# =========================================
# data_pipeline.py â€” ç”Ÿæˆ dataset_cleaned/
# =========================================
import os
import json
import numpy as np
from pathlib import Path
from sklearn.preprocessing import StandardScaler
import shutil

# ---------- è·¯å¾„è®¾ç½® ----------
SOURCE_DIR = Path("dataset")         # åŸå§‹æ•°æ®
OUTPUT_DIR = Path("dataset_cleaned") # æ¸…æ´—åè¾“å‡º

# ---------- å·¥å…·å‡½æ•° ----------
def ensure_dir(path: Path):
    os.makedirs(path, exist_ok=True)

def clean_data(X, y):
    """ç®€å•æ¸…æ´—ï¼šå»é™¤NaNå¹¶æ ‡å‡†åŒ–"""
    mask = ~np.isnan(X).any(axis=1)
    X, y = X[mask], y[mask]

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    return X_scaled, y

def process_split(split):
    print(f"ğŸ§¹ å¤„ç† {split} æ•°æ®é›†...")

    src_labels = SOURCE_DIR / split / "labels"
    dst_labels = OUTPUT_DIR / split / "labels"
    ensure_dir(dst_labels)

    X = np.load(src_labels / f"X_{split}.npy")
    y = np.load(src_labels / f"y_{split}.npy")
    with open(src_labels / f"{split}_labels.json", "r", encoding="utf-8") as f:
        label_info = json.load(f)

    print(f"åŸå§‹å½¢çŠ¶: {X.shape}")
    X_clean, y_clean = clean_data(X, y)
    print(f"æ¸…æ´—åå½¢çŠ¶: {X_clean.shape}")

    np.save(dst_labels / f"X_{split}.npy", X_clean)
    np.save(dst_labels / f"y_{split}.npy", y_clean)
    with open(dst_labels / f"{split}_labels.json", "w", encoding="utf-8") as f:
        json.dump(label_info, f, indent=4, ensure_ascii=False)

    # å¤åˆ¶ images æ–‡ä»¶å¤¹
    src_images = SOURCE_DIR / split / "images"
    dst_images = OUTPUT_DIR / split / "images"
    if src_images.exists():
        shutil.copytree(src_images, dst_images, dirs_exist_ok=True)

# ---------- ä¸»ç¨‹åº ----------
def main():
    print("=== ğŸš€ å¯åŠ¨æ•°æ®æ¸…æ´— ===")
    ensure_dir(OUTPUT_DIR)

    for split in ["train", "val", "test"]:
        process_split(split)

    print(f"\nâœ… æ•°æ®æ¸…æ´—å®Œæˆï¼æ–°æ•°æ®ä½äº: {OUTPUT_DIR.resolve()}")

if __name__ == "__main__":
    main()
